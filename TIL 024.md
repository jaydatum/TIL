# TIL 24

# 데이터 분석 부트캠프 45일차

# NLP 과정 시작

## NLP 개요

- 자연어  : 사람들이 일상적으로 쓰는 언어
- NLP(Natural Language Processing)
  - NLU :  사람이 이해하는 자연어를 컴퓨터가 이해할 수 있는 값으로 바꾸는 과정
  - NLG : 컴퓨터가 이해할 수 있는 값을 사람이 이해하도록 바꾸는 과정

![이미지 233](https://user-images.githubusercontent.com/98443610/154918062-c75b1f71-f345-49f0-abd6-5e0271a121fe.png)

##  Traditional NLP VS NLP with Deep Learning

Traditional NLP (전통적 통계기간 자연어 처리)

- count 기반

- 단어를 symbolic 데이터 (원핫 벡터)로 취급 (빨강, 분홍, 파랑은 모두 다름)
- 여러 서브 모듈들로 전체 모델이 구성 → 각각의 모듈을 지날 때마다 에러가 누적됨 

NLP with Deep Learning (딥러닝 기반 자연어 처리)

- 단어를 continuous value로 변환 (빨강과 분홍은 비슷)
- End-to-end 시스템 추구 → 하나의 모델로 구성되어 에러 누적 x

|                | 통계 기반 nlp           | 딥러닝 기반 nlp                          |
| -------------- | ----------------------- | ---------------------------------------- |
| 단어 벡터화    | 이산적, 심볼릭 공간     | 연속적, 신경망 공간                      |
| 사람인지       | 인지하기 쉬움           | 어려움                                   |
| 디버그         | 용이                    | 어려움                                   |
| 연산속도       | 느림                    | 빠름                                     |
| 모호성, 유의성 | 취약                    | 강함                                     |
| 모델구성       | 서브 모듈의 폭포수 구성 | end-to-end 모델, 성능개선, 시스템 간소화 |
| 기타           | 무거움                  |                                          |

## NLP vs 다른 분야

| NLP                                                          | Other Fields (CV, 음성인식)                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Discrete value 를 다룸 (단어, 문장)                          | continuous value를 다룸(이미지, 음성)                        |
| 분류 문제로 접근할 수 있음                                   | 문제에 따라 접근 방식이 다름                                 |
| 샘플의 확률값을 구할 수 있음 <br />(문장에서 이 단어가 나올 확률 드을 통해 자연스스러운 문장 찾기 가능) | 샘플의 확률값을 구할 수 없음<br />(연속 변수는 구간이 주어져야 확률값 계산 가능) |
| 문장생성(NLG) <br />auto-regressive한 속성을 지님<br />GAN 적용 불가 | 이미지 생성<br /> auto-regressive 속성 없음<br /> GAN 적용 가능 |

## NLP 특징

- 도메인 지식
  - 언어적인 지식 필요 : 한국어, 영어는 어떤 언어적 특성을 가지는가
  - 어떤 텍스트를 분석할 것인가
- Nasty Preprocesiing
  - 자연어 처리는 전처리가 매우 지저분하다 (이모티콘, 특수문자) : 단순하고 반복적인 과정이지만 미세한 차별화 포인트를 집중할 수 있는 부분이다.
  - Task에 따른 정제(normalization) 과정 필요

## NLP를 어렵게 하는 요소들

- Ambiguity  (모호성)
  - 중의성로 인한 모호성 → 해당 단어의 주변 단어를 확인해서 중의성 해소 (word sense)
  - 문장 내 정보 부족 → 추가적인 정보 제공으로 모호성 해소
- Paraphrase (다른 말로 바꾸어  표현한 문장 : 의역)
  - 수 많은 표현 형식으로 비슷한 의미의 단어들이 존재하는 paraphrase 문제가 존재
- Discrete, not continuous (이산값)
  - <u>one-hot 인코딩 값이 매우 sparse : 유사도, 모호성 표현이 불가능 / cv에서의 이미지 등은 유사</u>
  - '파랑'과 '핑크' 중 '빨강'에 가까운 단어는 무엇인가? , 계층적 구조를 가지는 어휘체계는 어떻게 표현하는가
  - 딥러닝 에서는 이 문제를 <b>word-embedding </b>으로 해결!!!

## 한국어 NLP의 어려운 점

- 교착어 : 접사 추가에 따라 의미가 파생 → 토큰화를 통해 원형과 접사 분리 필요
- 교착어의 유연한 어순(단어 순서) 규칙 특성
- 모호한 띄어쓰기
- 평서문과 의문문의 차이 부재
- 주어 부재
- 한자 기반의 언어 
- 단어 중의 성으로 인한 문제 발생

## Neural NLP의 역사

1. 딥러닝 이전 : 전형적인 NLP application (여러 단게의 sub-module) , traditional SMT
2. Sequence-to-Sequence 이전 : 문장이 주어지면 숫자표 변환 (단순 벡터 변환)
   - Word Embedding(word2vec)
   - Text Classification with CNN
3. Sequence-to-Sequence with Attention 이후 : seq2seq 모델에 attention이 더해셔 text to text 가능해짐
   - Beyond 'text to numeric'
   - Beyond 'text to numeric'
4. Era of Attention : Attention 원리를 응용한 Transformer 기반의 여러 서비스들이 등장
   - Transformer by End-to-End Attention
   - GPT-2, BERT, TRANSFORMER XL 모두 Attention 원리 기반
5. Pretraining and Fine-tuning : BERTology  → Big language models mainly based on Transformer
   - GPT 1,2,3,4 (Open AI)

## 최근 트렌드

![이미지 234](https://user-images.githubusercontent.com/98443610/154918312-fb321783-83b2-45c2-8693-15199675cffc.png)





